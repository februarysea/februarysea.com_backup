---
title: 机器学习（二）逻辑回归
date: 2019-11-15 14:32:33
categories: 学习笔记
tags:
- Machine Learning
---

该系列内容是基于吴恩达老师的[机器学习](https://www.coursera.org/learn/machine-learning/)课程笔记。

参考了[@fengdu78](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)的课程笔记开源项目。

***

#### 分类问题（Classifiction)

在分类问题中，要预测的变量 $y$ 是离散的值，将学习一种叫做逻辑回归 (**Logistic Regression**) 的算法，这是目前最流行使用最广泛的一种学习算法。

在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。

我们将因变量(**dependent variable**)可能属于的两个类分别称为负向类（**negative class**）和正向类（**positive class**），则因变量$y\in \{ 0,1 \}$ ，其中 0 表示负向类，1 表示正向类。

![](https://raw.githubusercontent.com/februarysea/picbed/master/f86eacc2a74159c068e82ea267a752f7.png)

如果我们要用线性回归算法来解决一个分类问题，对于分类， $y$ 取值为 0 或者1，但如果使用的是线性回归，那么函数的输出值可能远大于 1，或者远小于0。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。

顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使人感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 $y$ 取值**离散**的情况，如：1 0 0 1。

#### 逻辑回归（Logistic Regression）

根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：

当${h_\theta}\left( x \right)>=0.5$时，预测 $y=1$。

当${h_\theta}\left( x \right)<0.5$时，预测 $y=0$ 。

我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是： $h_\theta \left( x \right)=g\left(\theta^{T}X \right)$ 其中： $X$ 代表特征向量 $g$ 代表逻辑函数（**logistic function**)是一个常用的逻辑函数为**S**形函数（**Sigmoid function**），公式为： $g\left( z \right)=\frac{1} {1+{ {e}^{-z} } }$。

![](https://raw.githubusercontent.com/februarysea/picbed/master/1073efb17b0d053b4f9218d4393246cc.jpg)

合起来，我们得到逻辑回归模型的假设：

对模型的理解： $g\left( z \right)=\frac{1} {1+{ {e}^{-z} } }$。

$h_\theta \left( x \right)$的作用是，对于给定的输入变量，根据选择的参数计算输出**变量=1**的可能性（**estimated probablity**）即$h_\theta \left( x \right)=P\left( y=1|x;\theta \right)$ 例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\theta \left( x \right)=0.7$，则表示有70%的几率$y$为正向类，相应地$y$为负向类的几率为1-0.7=0.3。

#### 代价函数（Cost Function）

对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将${h_\theta}\left( x \right)=\frac{1} {1+{e^{-\theta^{T}x} } }$带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（**non-convexfunction**）。

![](https://raw.githubusercontent.com/februarysea/picbed/master/8b94e47b7630ac2b0bcb10d204513810.jpg)

这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

线性回归的代价函数为：$J\left( \theta \right)=\frac{1} {m}\sum\limits_{i=1}^{m} {\frac{1} {2} { {\left( {h_\theta}\left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} } }$ 。 我们重新定义逻辑回归的代价函数为：$J\left( \theta \right)=\frac{1} {m}\sum\limits_{i=1}^{m} { {Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$，其中

![](https://raw.githubusercontent.com/februarysea/picbed/master/54249cb51f0086fa6a805291bf2639f1.png)

${h_\theta}\left( x \right)$与 $Cost\left( {h_\theta}\left( x \right),y \right)$之间的关系如下图所示：

![](https://raw.githubusercontent.com/februarysea/picbed/master/ffa56adcc217800d71afdc3e0df88378.jpg)

这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：当实际的 $y=1$ 且${h_\theta}\left( x \right)$也为 1 时误差为 0，当 $y=1$ 但${h_\theta}\left( x \right)$不为1时误差随着${h_\theta}\left( x \right)$变小而变大；当实际的 $y=0$ 且${h_\theta}\left( x \right)$也为 0 时代价为 0，当$y=0$ 但${h_\theta}\left( x \right)$不为 0时误差随着 ${h_\theta}\left( x \right)$的变大而变大。 将构建的 $Cost\left( {h_\theta}\left( x \right),y \right)$简化如下： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ 带入代价函数得到： $J\left( \theta \right)=\frac{1} {m}\sum\limits_{i=1}^{m} {[-{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ 即：$J\left( \theta \right)=-\frac{1} {m}\sum\limits_{i=1}^{m} {[{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$

在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：

**Repeat** { $\theta_j := \theta_j - \alpha \frac{\partial} {\partial\theta_j} J(\theta)$ (**simultaneously update all** ) }

所以： $\frac{\partial } {\partial {\theta_{j} } }J\left( \theta \right)=\frac{\partial } {\partial {\theta_{j} } }[-\frac{1} {m}\sum\limits_{i=1}^{m} {[-{ {y}^{(i)} }\log \left( 1+{ {e}^{-{\theta^{T} } { {x}^{(i)} } } } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{ {\theta^{T} } { {x}^{(i)} } } } \right)]}]$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} {[-{ {y}^{(i)} }\frac{-x_{j}^{(i)} { {e}^{-{\theta^{T} } { {x}^{(i)} } } } } {1+{ {e}^{-{\theta^{T} } { {x}^{(i)} } } } }-\left( 1-{ {y}^{(i)} } \right)\frac{x_j^{(i)} { {e}^{ {\theta^T} { {x}^{(i)} } } } } {1+{ {e}^{ {\theta^T} { {x}^{(i)} } } } } }]$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} { {y}^{(i)} }\frac{x_j^{(i)} } {1+{ {e}^{ {\theta^T} { {x}^{(i)} } } } }-\left( 1-{ {y}^{(i)} } \right)\frac{x_j^{(i)} { {e}^{ {\theta^T} { {x}^{(i)} } } } } {1+{ {e}^{ {\theta^T} { {x}^{(i)} } } } }]$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} {\frac{ { {y}^{(i)} }x_j^{(i)}-x_j^{(i)} { {e}^{ {\theta^T} { {x}^{(i)} } } }+{ {y}^{(i)} }x_j^{(i)} { {e}^{ {\theta^T} { {x}^{(i)} } } } } {1+{ {e}^{ {\theta^T} { {x}^{(i)} } } } } }$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} {\frac{ { {y}^{(i)} }\left( 1\text{+} { {e}^{ {\theta^T} { {x}^{(i)} } } } \right)-{ {e}^{ {\theta^T} { {x}^{(i)} } } } } {1+{ {e}^{ {\theta^T} { {x}^{(i)} } } } }x_j^{(i)} }$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} {({ {y}^{(i)} }-\frac{ { {e}^{ {\theta^T} { {x}^{(i)} } } } } {1+{ {e}^{ {\theta^T} { {x}^{(i)} } } } })x_j^{(i)} }$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} {({ {y}^{(i)} }-\frac{1} {1+{ {e}^{-{\theta^T} { {x}^{(i)} } } } })x_j^{(i)} }$ $=-\frac{1} {m}\sum\limits_{i=1}^{m} {[{ {y}^{(i)} }-{h_\theta}\left( { {x}^{(i)} } \right)]x_j^{(i)} }$ $=\frac{1} {m}\sum\limits_{i=1}^{m} {[{h_\theta}\left( { {x}^{(i)} } \right)-{ {y}^{(i)} }]x_j^{(i)} }$

计算后得到了这个等式： ${\theta_j}:={\theta_j}-\alpha \frac{1} {m}\sum\limits_{i=1}^{m} {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} }){x_{j} }^{(i)} }$ ，将后面这个式子，在 $i=1$ 到 $m$ 上求和，其实就是预测误差乘以$x_j^{(i)}$ ，所以你把这个偏导数项$\frac{\partial } {\partial {\theta_j} }J\left( \theta \right)$放回到原来式子这里，我们就可以将梯度下降算法写作如下形式： ${\theta_j}:={\theta_j}-\alpha \frac{1} {m}\sum\limits_{i=1}^{m} {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} }){x_{j} }^{(i)} }$

所以，最终的的梯度下降法：

**Repeat** { $\theta_j := \theta_j - \alpha \frac{1} {m}\sum\limits_{i=1}^{m} { {\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)} }\mathop{x}_{j}^{(i)}$ **(simultaneously update all** ) }

注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。

#### 多类分类问题（Multiclass Classification_ One-vs-all）

对于之前的一个二元分类问题，我们的数据看起来可能如左图所示，然而对于一个多类分类问题，我们的数据集或许看起来像右图一样。

![](https://raw.githubusercontent.com/februarysea/picbed/master/54d7903564b4416305b26f6ff2e13c04.png)



下面将介绍如何进行一对多的分类工作，有时这个方法也被称为"一对余"方法。

![](https://raw.githubusercontent.com/februarysea/picbed/master/b72863ce7f85cd491e5b940924ef5a5f.png)

这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。

为了能实现这样的转变，我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地第我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。 最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3....k \right)$

最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。

总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_\theta^{\left( i \right)}\left( x \right)$， 其中 $i$ 对应每一个可能的 $y=i$，最后，为了做出预测，我们给出输入一个新的 $x$ 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 $x$，然后我们选择一个让 $h_\theta^{\left( i \right)}\left( x \right)$ 最大的$ i$，即$\mathop{\max}\limits_i,h_\theta^{\left( i \right)}\left( x \right)$。

现在我们知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论$i$值是多少，我们都有最高的概率值，我们预测$y$就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，我们也可以将逻辑回归分类器用在多类分类的问题上。

#### 正则化

到现在为止，我们已经学习了几种不同的学习算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(**over-fitting**)的问题，可能会导致它们效果很差。

如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。

![](https://raw.githubusercontent.com/februarysea/picbed/master/72f84165fbf1753cd516e65d5e91c0d3.jpg)

第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。

就以多项式理解，$x$ 的次数越高，拟合的越好，但相应的预测的能力就可能变差。

问题是，如果我们发现了过拟合问题，应该如何处理？

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
2. 正则化。 保留所有的特征，但是减少参数的大小（**magnitude**）。

#### 正则化线性回归

对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。

正则化线性回归的代价函数为：

$J\left( \theta \right)=\frac{1} {2m}\sum\limits_{i=1}^{m} {[({ {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })}^{2} }+\lambda \sum\limits_{j=1}^{n} {\theta _{j}^{2} })]}$

如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以梯度下降算法将分两种情形：

$Repeat$ $until$ $convergence${

 ${\theta_0}:={\theta_0}-a\frac{1} {m}\sum\limits_{i=1}^{m} {(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{0}^{(i)} })$

 ${\theta_j}:={\theta_j}-a[\frac{1} {m}\sum\limits_{i=1}^{m} {(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }+\frac{\lambda } {m} {\theta_j}]$

 $for$ $j=1,2,...n$

 }

对上面的算法中$ j=1,2,...,n$ 时的更新式子进行调整可得：

${\theta_j}:={\theta_j}(1-a\frac{\lambda } {m})-a\frac{1} {m}\sum\limits_{i=1}^{m} {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }$ 可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta $值减少了一个额外的值。

#### 正则化逻辑回归

针对逻辑回归问题，在之前学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数$J\left( \theta \right)$，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数$J\left( \theta \right)$。

![](https://raw.githubusercontent.com/februarysea/picbed/master/2726da11c772fc58f0c85e40aaed14bd.png)

自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：

$J\left( \theta \right)=\frac{1} {m}\sum\limits_{i=1}^{m} {[-{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}+\frac{\lambda } {2m}\sum\limits_{j=1}^{n} {\theta _{j}^{2} }$

要最小化该代价函数，通过求导，得出梯度下降算法为：

$Repeat$ $until$ $convergence${

 ${\theta_0}:={\theta_0}-a\frac{1} {m}\sum\limits_{i=1}^{m} {(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{0}^{(i)} })$

 ${\theta_j}:={\theta_j}-a[\frac{1} {m}\sum\limits_{i=1}^{m} {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }+\frac{\lambda } {m} {\theta_j}]$

 $for$ $j=1,2,...n$

 }

注意：

1. 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者的${h_\theta}\left( x \right)$不同所以还是有很大差别。
2. ${\theta_{0} }$不参与其中的任何一个正则化。
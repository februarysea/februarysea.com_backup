---
title: 生成微博词云
date: 2019-10-30 19:15:22
categories: 项目记录
tags:
- crawler
- python
---

今天花了一天写了一个新浪微博的爬虫，爬取微博指定用户微博内容，然后生成词云，以[@带带大师兄]([https://weibo.com/u/3176010690?from=feed&loc=at&nick=%E5%B8%A6%E5%B8%A6%E5%A4%A7%E5%B8%88%E5%85%84&is_all=1](https://weibo.com/u/3176010690?from=feed&loc=at&nick=带带大师兄&is_all=1))为例，这是带带大师兄微博的图云。

![](https://raw.githubusercontent.com/februarysea/picbed/master/%E5%B8%A6%E5%B8%A6%E5%A4%A7%E5%B8%88%E5%85%84.png)

具体实现思路是：手机端网页的微博内容比较容易获取，于是通过爬虫访问手机端微博网页`m.weibo.com`获取某人的微博信息，然后将微博信息构成一个字符串进行词语分割，最后用分割的词语生成词云。

* 构建请求头：主要是为了微博把我们的爬虫识别为浏览器。

  ```python
  headers = {
              'Host': "m.weibo.cn",
              'Referer': "https://www.baidu.com/",
              'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
              'X-Requested-With': "XMLHttpRequest",
          }
  ```

* 构建具体链接

  以带带大师兄为例，在他的微博页面向下滑动，打开开发者工具界面，可以看到不断有`XHR`链接被请求，这样我们就得到了带带大师兄微博真正请求链接。

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/Screen%20Shot%202019-10-31%20at%2010.15.55%20AM.png)

  分析链接不难得到有`type`、`uid`、`containerid`这几个参数，如果看后面的XHR请求的话还有`page`这个参数，于是我们的请求参数构建如下所示：

  ```python
  params = {
      "type": "uid",
      "value": uid,
      "containerid": containerid,
    	"page": 1
  }
  ```

  不难发现`uid`就是微博主页链接中后面的一串数字，`containerid`其实就是`10763+uid`，所以我们的请求构建如下：

  ```python
          containerid = str(107603) + uid
          params = {
              "type": "uid",
              "value": uid,
              "containerid": containerid,
            	"page": 1
          }
        	baseUrl = "https://m.weibo.cn/api/container/getIndex?" + urlencode(params)
  ```

  这里使用了`urlencode`是`urllib.parse `库的一个函数，它将参数编码为符合访问规则的URL。我们最后得到的URL：`	https://m.weibo.cn/api/container/getIndex?type=uid&value=3176010690&containerid=1076033176010690&page=1`

* 获取微博内容

  在这里通过分析请求返回的`json`内容，不难得到微博内容是在`card['mblog']['text']`中，通过json库的解析，获得具体的微博内容，然后针对处理各种情况进行一个不太严谨的过滤，就得到了我们想要的微博内容，并把所有内容构建成一个字符串，当然这里也可以逐条存储到数据库中，因为这里微博量不大，就没有这么做。

  ```python
              # 获取微博内容
              try:
                  time.sleep(random.random() * 3)  # 随机延时0-3s
                  response = requests.get(url, headers=headers)
                  if response.status_code == 200:
                      json = response.json()
                      cards = json['data']['cards']
                      for card in cards:
                          try:
                              text = card['mblog']['text']
                          except Exception as e:
                              print("error: ", e)
                              continue
  
                          #  未显示完全的情况
                          match = re.search(pattern="<a href=\"/status/[0-9]+\">全文</a>", string=text)
                          if match:
                              reMatch = re.search(pattern="[0-9]+", string=match.group(0))
                              params = {
                                  "id": reMatch.group(0)
                              }
                              url = "https://m.weibo.cn/statuses/extend?" + urlencode(params)
                              try:
                                  time.sleep(random.random() * 3)  # 随机延时0-3s
                                  response = requests.get(
                                      url=url,
                                      headers=headers)
                                  if response.status_code == 200:
                                      json = response.json()
                                      text = json['data']['longTextContent']
                              except requests.ConnectionError as e:
                                  print("Error", e.args)
  
                          # 过滤空行
                          text = re.sub(pattern="<br />|<span.*>", repl="", string=text)
                          # 过滤投票
                          text = re.sub(pattern="\u6211\u53c2\u4e0e.*", repl="", string=text)
                          # 过滤围观问答
                          text = re.sub(pattern="\u6211\u514d\u8d39\u56f4\u89c2\u4e86.*", repl="", string=text)
                          # 过滤链接/话题
                          text = re.sub(pattern="<a .*>", repl="", string=text)
  
                          # 过滤转发微博/分享图片/经之前过滤成为空行的内容
                          if text == "转发微博":
                              continue
                          elif text == "分享图片 ":
                              continue
                          elif text == " ":
                              continue
                          elif text =="":
                              continue
                          text = re.sub(pattern=" *", repl="", string=text)
                          self.words = self.words + text
                          print(text)
              except requests.ConnectionError as e:
                  print('Error', e.args)
  ```

* 生成词云

  针对获得的内容，使用`jieba`库进行中文词意的分割，然后调用用`wordcloud`库即可生成词云。

  这两个库具体的使用还是得看他们的文档，我在这里也就不具体说明了。

  jieba:https://github.com/fxsjy/jieba

  word_cloud:https://github.com/amueller/word_cloud

  有一点要注意的是，对于中文词云的生成`font_path="simfang.ttf"`,这一行一定不能少，否则无法生成。

  ```python
          # 精确模式分割
          text = jieba.cut(self.words)
          # 空格分词
          text = " ".join(text)
          # 生成词云
          wordcloud = WordCloud(
              background_color="white",
              font_path="simfang.ttf",
              width=800,
              height=600)
          wordcloud.generate(text)
          wordcloud.to_file("result/"+self.user+".png")
          plt.imshow(wordcloud)
          plt.axis("off")
          plt.show() 
  ```

***

具体的代码我已经放在了[这里](https://github.com/februarysea/wordcloud)，欢迎star。

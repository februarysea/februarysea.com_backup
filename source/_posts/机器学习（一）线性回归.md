---
title: 机器学习（一）线性回归
date: 2019-10-29 16:06:13
categories: 学习笔记
tags:
- Machine Learning
---

该系列内容是基于吴恩达老师的[机器学习](https://www.coursera.org/learn/machine-learning/)课程笔记。

参考了[@fengdu78](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)的课程笔记开源项目。

***

#### 监督学习（Supervised Learning）和非监督学习(Unsupervised Learning)

* 监督学习：对于监督学习中的每一个样本，我们已经被清楚地告知了，什么是所谓的正确答案，对于预测肿瘤来说，样本病人年龄等参数和肿瘤是良性还是恶性已经确定，对于预测房价来说，样本的房屋面积等参数对应的房价是确定的。我们算法通过找到最符合样本的一个函数。通过这个函数来预测其他情况。

  ![](https://github.com/februarysea/picbed/blob/master/supervised.png?raw=true)

  ![](https://github.com/februarysea/picbed/blob/master/supervisedlearning2.png?raw=true)

* 无监督学习：在无监督学习中，没有属性或标签这一概念，也就是说所有的数据，都是一样的。我们要依靠算法把相似的数据样本放到一起，把不同的样本区分开。比如，许多公司拥有庞大的客户信息数据库，那么给你一个客户数据集，你能否自动找出不同的市场分割，并自动将你的客户分到不同的细分市场中，从而有助于我在不同的细分市场中进行更有效的销售。我们现在有这些客户数据，但我们预先并不知道有哪些细分市场，而且，对于我们数据集的某个客户，我们也不能预先知道，谁属于细分市场一，谁又属于细分市场二等等，但我们必须让这个算法自己去从数据中发现这一切。

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/0c93b5efd5fd5601ed475d2c8a0e6dcd.png)

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/94f0b1d26de3923fc4ae934ec05c66ab.png)

### 单变量线性回归（Linear Regression with One Variable）

我们将要用来描述这个回归问题的标记如下:

$m$ 代表训练集中实例的数量

$x$ 代表特征/输入变量

$y$ 代表目标变量/输出变量

$\left( x,y \right)$ 代表训练集中的实例

$({ {x}^{(i)} },{ {y}^{(i)} })$ 代表第$i$ 个观察实例

$h$ 代表学习算法的解决方案或函数也称为假设（**hypothesis**）

一种可能的表达方式为：$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。

![](https://raw.githubusercontent.com/februarysea/picbed/master/H6qTdZmYEeaagxL7xdFKxA_2f0f671110e8f7446bb2b5b2f75a8874_Screenshot-2016-10-23-20.14.58.png)

为了预测尽可能的精确，我们使预测值$h_\theta \left( x \right)$和实际值$y$之间的差距尽可能的小，即 求代价函数**（Cost Function）**：$J \left( \theta_0, \theta_1 \right) = \frac{1} {2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$的最小值。其中，指数上的$2$是为了防止出现负值，$\frac{1} {2m}$是为了后面的梯度下降法计算方便。

* **梯度下降法**

  使用梯度下降法求解代价函数$J \left( \theta_0, \theta_1 \right) = \frac{1} {2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$的最小值，

  主要思想是：

  1. 给$ \theta_0, \theta_1$一个初值，通常设为$0$
  2. 改变$ \theta_0, \theta_1$，来减小$J \left( \theta_0, \theta_1 \right)$直到找到$J \left( \theta_0, \theta_1 \right)$的最小值或局部最小值

  梯度下降法的更新规则如下：

  ​									${\theta_{j} }:={\theta_{j} }-\alpha \frac{\partial } {\partial {\theta_{j} } }J\left( \theta \right)$，

  其中，$a$是学习率，通常要根据设计算法的工程师的经验去设计。

  如果$a$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$a$太大，它会导致无法收敛，甚至发散。

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/UJpiD6GWEeai9RKvXdDYag_3c3ad6625a2a4ec8456f421a2f4daf2e_Screenshot-2016-11-03-00.05.27.png)

  值得注意的是，每个${\theta_{j} }$都需要同时更新，如图所示。

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/yr-D1aDMEeai9RKvXdDYag_627e5ab52d5ff941c0fcc741c2b162a0_Screenshot-2016-11-02-00.19.56.png)

### 多变量线性回归（Linear Regression with Multiple Variable）

以房价模型为例，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为$\left( {x_{1} },{x_{2} },...,{x_{n} } \right)$。

![](https://raw.githubusercontent.com/februarysea/picbed/master/591785837c95bca369021efa14a8bb1c.png)

增添更多特征后，我们引入一系列新的注释：

$n$ 代表特征的数量

${x^{\left( i \right)} }$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个**向量**（**vector**）。

比方说，上图的

${x}^{(2)}\text{=}\begin{bmatrix} 1416\\ 3\\ 2\\ 40 \end{bmatrix}$，

${x}_{j}^{\left( i \right)}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。

如上图的$x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2$，

支持多变量的假设 $h$ 表示为：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} } {x_{1} }+{\theta_{2} } {x_{2} }+...+{\theta_{n} } {x_{n} }$，

这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} \left( x \right)={\theta_{0} } {x_{0} }+{\theta_{1} } {x_{1} }+{\theta_{2} } {x_{2} }+...+{\theta_{n} } {x_{n} }$

此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} \left( x \right)={\theta^{T} }X$，其中上标$T$代表矩阵转置。

* **多变量梯度下降法**

  与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：$J\left( {\theta_{0} },{\theta_{1} }...{\theta_{n} } \right)=\frac{1} {2m}\sum\limits_{i=1}^{m} { { {\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} } }$ ，

  其中：$h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0} }+{\theta_{1} } {x_{1} }+{\theta_{2} } {x_{2} }+...+{\theta_{n} } {x_{n} }$ ，

  我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度下降算法为：

  ​													${\theta_{j} }:={\theta_{j} }-\alpha \frac{\partial } {\partial {\theta_{j} } }J\left( \theta_{0},\theta_{1}...\theta_{n}\right) $

  

* **梯度下降法实践1-特征缩放**

  在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

  以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/966e5a9b00687678374b8221fdd33475.jpg)

  解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/b8167ff0926046e112acf789dba98057.png)

  最简单的方法是令：${ {x} }=\frac{ {x}-{\mu} } { {s} }$  ，其中${\mu_{n} }$是平均值，${s_{n} }$是标准差。

* **梯度下降法实践2-学习率**

  梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

  通常可以考虑尝试些学习率：

  ​														$\alpha=0.01，0.03，0.1，0.3，1，3，10$

* **多项式回归**

  以房价预测问题为例，现在有如下图所示两个参数。

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/8ffaa10ae1138f1873bc65e1e3657bd4.png)

  $h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }\times{frontage}+{\theta_{2} }\times{depth}$

  ${x_{1} }=frontage$（临街宽度），${x_{2} }=depth$（纵向深度），$x=frontage*depth=area$（面积），则：${h_{\theta} }\left( x \right)={\theta_{0} }+{\theta_{1} }x$。 线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} } {x_{1} }+{\theta_{2} } {x_{2}^2}$ 或者三次方模型： $h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} } {x_{1} }+{\theta_{2} } {x_{2}^2}+{\theta_{3} } {x_{3}^3}$

  ![](https://raw.githubusercontent.com/februarysea/picbed/master/3a47e15258012b06b34d4e05fb3af2cf.jpg)

  

  注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

* **正规方程**

  正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}   {\partial{\theta_{j} } }J\left( {\theta_{j} } \right)=0$ 。 假设我们的训练集特征矩阵为 $X$（包含了 ${ {x}_{0} }=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量： 

  ​															$\theta ={ {\left( {X^T}X \right)}^{-1} } {X^{T} }y$ 

  上标**T**代表矩阵转置，上标-1 代表矩阵的逆。

  

  注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。

  梯度下降与正规方程的比较：

  | 梯度下降                      | 正规方程                                                     |
  | ----------------------------- | ------------------------------------------------------------ |
  | 需要选择学习率$\alpha$        | 不需要                                                       |
  | 需要多次迭代                  | 一次运算得出                                                 |
  | 当特征数量$n$大时也能较好适用 | 需要计算${ {\left( { {X}^{T} }X \right)}^{-1} }$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O\left( { {n}^{3} } \right)$，通常来说当$n$小于10000 时还是可以接受的 |
  | 适用于各种类型的模型          | 只适用于线性模型，不适合逻辑回归模型等其他模型               |

  总结一下，只要特征变量的数目并不大，正规方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，通常使用正规方程法，而不使用梯度下降法。

  
